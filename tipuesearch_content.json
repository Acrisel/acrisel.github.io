{"pages":[{"url":"posts/2017/09/python-lex-yacc-play/","title":"Python Lex and Yacc with PLY","tags":"09","text":"Grammer Reusability Synopsis In many cases, grammar is used in a singular scope of actions. That is, any sentence in the grammar yields distinct action. In some cases, we want to produce different outcomes in different compiling passes of the same text. This post focuses on how to use Python/PLY to accomplish just that using the same grammar definitions. A normal course of action is to duplicate the grammar definitions and change the action associated with its rules. This creates a situation where the grammar needs to be maintained in multiple locations. The idea of grammar reusability lies in the ability to bind different actionable objects to the same grammar definitions. For that, actions are defined externally to the grammar. Each parser pass will provide PLY with its unique action objects. Definitions To remove confusion, let's define some ground definitions. Grammer rule: YACC based rule definition; e.g., add : x '+' rest; Grammer definition: set of grammer rules defining a language Motivation We were using PLY to write a parser for Cobol. That was done to analyze code and to produce artifacts that would be used in programming interfaces to Cobol programs. Very quickly we ran into the need to apply different actions to grammar rules according to the parsing that we needed to do. As usually the case in modern programming practices - namely lazy programming, developers copy the grammar and change the action associated with rules according to their need. Then developers run into situations where a grammar rule was found buggy and needed to change. Now they had to change multiple grammar definition files. This naturally prompt the need to centralize grammar definition and to allow its reusability with different parsing actions. Design For grammar definitions to be come reusable, the rules needed to be separated from the action they apply. This devises three guiding principles. rules will need to be shaped to call a shared object that would perform their action. each instantiation of parser will include binding of an actionable object. an actionable object will be sensitive to the rule it is called upon. This discussion will use basic calculator called classcalc provided as an example in PLY. We will make changes to this example that will create it as reusable grammar. The complete code can be found in calcaction . Actionable Object An actionable object is a callable that defines the actions each grammar rule would perform. For this discussion, the object is scaled down. We will note however some feature that project will most probably want. General Actionable Object ParserActions is a base class defining basic actions for actionable objects. An object is instantiated using an action map (shown in CalcActions ). It callable method accepts PLY parser object ( p ), and key into action_map ( tag ). ___call__() uses tag to find the proper action. It then applies the action to parser object p . 1 2 3 4 5 6 7 8 9 10 11 class ParserActions ( object ): def __init__ ( self , action_map = {},): self . action_map = action_map def __call__ ( self , p , tag ): try : action = self . action_map [ tag ] except : try : action = self . action_map [ '' ] except : return p p [ 0 ] = action ( p ) Special Actionable Object Specializing parser action is done simply by inheriting from ParserActions and providing a map of actions. CalcActions does just that. It defined a mapping between rule tags and actions need to be performed on them. Action can be a simple lambda function or a more complicated mechanism implemented as a method. Since ParserActions uses the return from an action to set the rule return value p[0] , methods need to return value to their rule. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class CalcActions ( ParserActions ): def __init__ ( self ,): action_map = { 'statement_assign' : self . set_name , 'statement_expr' : self . print_result , 'expression_binop' : self . calc_expr , 'expression_uminus' : lambda p : - p [ 2 ], 'expression_group' : lambda p : p [ 2 ], 'expression_number' : lambda p : p [ 1 ], 'expression_name' : self . exp_name , } super () . __init__ ( action_map = action_map ) self . names = dict () def print_result ( self , p ): print ( p [ 1 ]) def set_name ( self , p ): self . names [ p [ 1 ]] = p [ 3 ] def calc_expr ( self , p ): if p [ 2 ] == '+' : p [ 0 ] = p [ 1 ] + p [ 3 ] elif p [ 2 ] == '-' : p [ 0 ] = p [ 1 ] - p [ 3 ] elif p [ 2 ] == '*' : p [ 0 ] = p [ 1 ] * p [ 3 ] elif p [ 2 ] == '/' : p [ 0 ] = p [ 1 ] / p [ 3 ] elif p [ 2 ] == '**' : p [ 0 ] = p [ 1 ] ** p [ 3 ] return p [ 0 ] def exp_name ( self , p ): try : p [ 0 ] = self . names [ p [ 1 ]] except LookupError : print ( \"Undefined name ' %s '\" % p [ 1 ]) p [ 0 ] = 0 return p [ 0 ] Grammer Hooks There are two steps to hook into the grammar. Pass actionable parser object to a parser. Let rules call that object to initiate action. Initiate Parser with Actions Call to parser will look as follows: 1 2 rule_action = CalcActions () calc = Calc ( rule_action = rule_action ) In parser initialization, rule_action is set so it can be used within parser object. 1 self . rule_action = rule_action Hook Rules Rules are hooked with actions by calling ParserAction specialized object, in calc case: CalcActions . The original p_statement_assign was defined follows: 1 2 3 def p_statement_assign ( self , p ): 'statement : NAME EQUALS expression' self . names [ p [ 1 ]] = p [ 3 ] With reusability mechanism, the method will be changed to call an actionable 1 2 3 def p_statement_assign ( self , p ): 'statement : NAME EQUALS expression' self . rule_action ( p , 'statement_assign' ) Call to rule_action passes the parser object and the rule's key to action_map . Change Actions To change actions, we can define a new actionable object with new action map. This is done without changing the grammar. For example ( CountOpsActions ), if we want to add a count of the number of operations when we print calc result, the following would be done. Create a new actionable class similar to the one before but with a counter self.count_ops_in_expr=0 initiated to 0. calc_expr() will advance the counter. print_result will be changed to print the counter and to rest its value for the next operation. The complete actionable object will look as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 class CountOpsActions ( CalcActions ): def __init__ ( self ,): super () . __init__ () self . count_ops_in_expr = 0 def print_result ( self , p ): print ( \"result: %s (ops count: %s )\" % ( p [ 1 ], self . count_ops_in_expr )) self . count_ops_in_expr = 0 def calc_expr ( self , p ): result = super () . calc_expr ( p ) self . count_ops_in_expr += 1 return result To make this work, we only need to initiate parser with the new actionable object. 1 2 rule_action = CountOpsActions () calc = Calc ( rule_action = rule_action ) In a more complicated scenarios, a complete new action can be provided. Additional Notes In the CalcAct example, only parser object is passed to the action. In a more complex grammar, it may be beneficial to structure action to also receive the tag. In close enough actions, it would be then possible to reuse same code with different actions using the tag as a differentiator. In the above example, ParserActions will initiate action with add tag . 1 p [ 0 ] = action ( p , tag ) Mapping actions will also add tag to the call. For example: 1 'expression_uminus' : lambda p , tag : - p [ 2 ], Callable methods will also need to change to accept tag . Some time a default action could also be introduced. In this case, an action with an empty key can be added. 1 '' : self . some_default_action , ParserActions as described above already built to accommodate such default action. Limitation If a new PLY object is created with new action map, it affects previously instantiated objects. Therefore, to work with multiple parser action sets in the same program, you have to re-initiate the parser each time action set is changed. Conclusion It is possible to override parser rules by inheriting from class rules and overriding rules of interest. Doing so still, results with replication of rules. The method shown here allows complete isolation of grammar rules from their actions. Hence allowing reusability of grammar code with different actions. References PLY by David Beazley classcalc by David McNab"},{"url":"posts/2017/08/pelican-post-template-controls/","title":"Pelican Post Template Controls","tags":"08","text":"How to selectively show comments block Synopsis This post will cover how to control templates from articles. In specific, how to enable or disable comments block in posts. Motivation We are using Pelican static website framework with Pelican Comment System . However, we wanted to disable for some of the articles the ability to show comments. How to There were three elements that needed to control commnets: Add DEFAULT_METADATA item named showcomments to pelicanconf.py : 1 DEFAULT_METADATA = { 'showcomments' : 'yes' ,} Add a condition to show comments block in theme's article.html: 1 2 3 { % if article . showcomments == \"yes\" % } {{ pcs . comments_quickstart ( \"support\" , \"acrisel.com\" ) }} { % endif % } Add :showcomments: no metadata to disable comments block in any article. 1 2 3 4 5 6 7 : date : 2017 - 07 - 09 10 : 20 : modified : 2017 - 07 - 09 18 : 40 : category : example : slug : pelican - selective - display - content : showcomments : no : authors : The Acrisel Team : summary : pelican selective display content That's it, done. Conclusion We showed here a method to dynamically control the display of comments block. This method can be used for other content elements. Others metadata tags can be added to bother DEFAULT_METADATA and articles. References Pelican Comment System by Bernhard Scheirle"},{"url":"posts/2017/08/ssh-made-easy-using-python/","title":"SSH Made Easy Using Python Subprocess","tags":"08","text":"Subprocess Module Synopsis Python's subprocess module makes it easy to invoke external commands on localhost. It can also use SSH to invoke commands on remote hosts. How simple? The short answer is, very simple! The longer answer is, subprocess.run, as follows: 1 2 3 4 5 result = subprocess . run ([ \"ssh\" , \"some-host@some-user\" , \"some-command\" ], shell = False , stdout = subprocess . PIPE , stderr = subprocess . PIPE , check = False ) Prerequisites For subprocess ssh-run to work, ssh configuration needs to be in place. This means that: ssh-keygen on originating machine ssh-copy-id to the target machine into the target account's .ssh/authorized_keys optional: create .ssh/config for easier ssh call optional: use command in target .ssh/authorized_keys to allow proper setup, if needed, e.g: 1 command \"if [[ \\\" x${SSH_ORIGINAL_COMMAND}x \\\" != \\\" xx \\\" ]]; then source ~/.bash; eval \\\" ${SSH_ORIGINAL_COMMAND} \\\" ; else /bin/bash --login; fi;\" < ssh_key > Wrapper 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def sshcmd ( host , command , user = None , stdin = None , check = False ): ''' Runs ssh command via subprocess. Assuming .ssh/config is configured. Args: host: target host to send the command to command: command to run on the host user: (optional) user to use to login to host stdin: (optional) override sys.stdin check: (optional) pass to *subprocess.run*; if set, checks return code and raises subprocess.CalledProcessError, if none-zero result Returns: subprocess.CompletedProcess object ''' where = \" %s \" % host if user is None else \" %s @ %s \" % ( user , host ) result = subprocess . run ([ \"ssh\" , where , command ], shell = False , stdin = stdin , stdout = subprocess . PIPE , stderr = subprocess . PIPE , check = check ) return result if __name__ == '__main__' : # will work on if you have a host named ubly with ssh configuration out = sshcmd ( \"ubly\" , \"ls -l\" , check = False ) . stdout . decode () Note the following: check=True would cause subprocess.run to through subprocess.CalledProcessError , if ssh exits with error code (greater than 0). sshcmd returns subprocess.CompletedProcess ; which gives full access to PIPEs and return-code . stdout and stderr attributes are of binary form. Therefore result needs to be decoded() ."},{"url":"posts/2017/08/astrophysics-scientific-python-programming/","title":"Astrophysics Research Python Programming","tags":"08","text":"The Productive Research Introduction Recently I helped a group of astrophysicists convert a set of IDL programs to Python. This set was to classify stars by analyzing ROTSE-I and ROTSE-III data. In this insert I would share the experience of moving IDL code to python. The insert is only attempt to cover interesting aspects that we encountered. At the same time, it serves as an encouragement to perform such migration. Python is a powerful tool. As a software architect I recognize the readability of Python programs, the vast libraries that one can tap on, and the large community that continues enrich its capabilities (scientific, WEB, network, you name it, its there). In fact, Python is being used for Data Analytics and Machine Learning; just google it around you will see many courses in the realm. So basically, with scipy , numpy , pandas , matplotlib , and more, one can accomplish everything done with IDL and more. From IDL to Python IDL is a \"scientific programming language used across disciplines to extract meaningful visualizations from complex numerical data. With IDL you can interpret your data, expedite discoveries, and deliver powerful applications to market.\" It uses plenty vector and matrix computation. So we will have a lot of that with numpy computing techniques. Loading FITS and MATCH The first we have to be able to read the ROTSE-I and ROTSE-III data files. There were three types of files. .dat: MATCH structure .datc: compressed MATCH structure .fit: FITS structure Luckily, all these structures are readable natively by python packages. The following code insert shows how to read MATCH and FITS structured files. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from scipy.io import readsav import pyfits def read_fits_file ( file , fits_index = 1 ): try : hdus = pyfits . open ( file , memmap = True ) hdus_ext = hdus [ fits_index ] data = hdus_ext . data except Exception as e : raise Exception ( \"cannot read fits data from file: %s \" % ( file ,)) from e return data def read_match_file ( file , * args , ** kwargs ): try : data = readsav ( file )[ 'match' ] except Exception as e : raise Exception ( \"cannot read match data from file: %s \" % ( file ,)) from e return data The above code uses two packages scipy.io and pyfits , which provides the actual loading of the structures. Note that readsav reads both .dat and .datc files. The type of the resulting data is of type numpy.recarry . Elements of recarray can be accessed via name either by the '.' or '[]' notation. It is easy to have a general purpose reader by having a routing function as follows: 1 2 3 4 5 6 7 8 9 10 def read_data_file ( file , fits_index = 1 , tmpdir = '/tmp' ): if not os . path . isfile ( file ): raise Exception ( \"file not found: %s \" % ( file ,)) file_ext = file . rpartition ( '.' )[ 2 ] if file_ext == 'fit' : data = read_fits_file ( file , fits_index ) else : # assume MATCH data = read_match_file ( file ) return data Obviously, this is good only if the next processing steps takes into account differences between the two structures. For example, MATCH structure have a STAT field that is not part of FITS structute. 1 2 try : stat = data . field ( 'STAT' )[ 0 ] except : stat = None # assume not a MATCH file Where statement IDL programs take advantage of IDL's where_ statement. It will look something like follows: 1 2 select_indexes = where ( data . flags [ * , ptr ] gt - 1 and $ check_flags ( ref_vflag , data . flags [ * , ptr ], type = 'STYPE' ) eq 0 ) The above statement is pretty loaded in functionality. In essence, the expected result are indexes into the vector data.values[ ,k] that adheres to two conditions: the need to be greater than -1; and the result for the function *check_value to be zero. 1 2 3 4 5 6 7 8 9 import numpy as np # assum data is a MATCH or FITS data structure # and that check_flag function is defined for ref_flag and flags = data . field ( 'FLAGS' )[ 0 ] check_flags_v = np . vectorize ( lambda flag : check_flag ( ref_flag , flag ) == 0 ) cond = np . logical_and . reduce ( ( flags [ ptr ,:] > - 1 , check_flags_v ( flags [ ptr ,:]), ), ) select_indexes = np . where ( cond ); select_indexes = select_indexes [ 0 ] To match IDL with Python, we are using four distinct numpy functions. vectorize transforms a vector by applying a function on its elements. logical_and and reduce : applys logical test to vector element transforming it to boolean vector. where : return indices of those element set to True. An important note is that IDL matrix indices are in opposite order to that of numpy. From here on we will assume np stands for numpy imported as np. Just to clarify, numpy performs operation on arrays. For example, assuming merr and msys are arrays of the same size, the following will produce a new array which each element is the square root of the squared sum of related elements. Obviously, a more sophisticated computation can be deployed. 1 np . sqrt ( merr ** 2.0 + msys ** 2.0 ) Hierarchical record array initialization IDL's create_struct_ is being used to create records with fields accessible by name. It's parallel in numpy realm is recarray . The following code set shows how to create recarray structure with default values. Starting with the definition of the records' fields, field_map. it is built as a list of tuple, each describe a name a field, a type, and a default value. 1 2 3 4 5 6 7 field_map = [ ( 'state' , int , - 1 ), ( 'distance' , np . float32 , - 1.0 ), ( 'posangle' , np . float32 , - 1.0 ), ( 'error' , np . float32 , - 1.0 ), ( 'phot' , np . float32 , - 1.0 ), ( 'photerror' , np . float32 , - 1.0 ),] Next, base on the above mapping, create_structured_vector would create the records' data type and a vector[size] with elements of the desired data type. 1 2 3 4 5 6 7 8 9 10 11 def create_structured_vector ( size , field_map , copy = False ): dts = list () for name , type_ , _ in field_map : dts . append ( ( name , type_ ,) ) dtype = np . dtype ( dts ) values = [ tuple ([ value if not copy else np . copy ( value ) for _ , _ , value in field_map ]) for _ in range ( size )] array = np . rec . array ( values , dtype = dtype ) return array , dtype The copy option of create_structured_vector , if set, tells it to copy the default values. This is useful in case the default value is a structure by itself. Note that the function returns a tuple of the generated array and the created type. This is useful in case further association of this data type is required. This method could be extended to handle any array shape, not just vectors. Plotting IDL's plot capability can be achieved using matplotlib and pandas's dataframe.plot. These tools are rich with features and easy to use. The original IDL code we were porting was built creating postscript documents. Using matplotlib we switched to PDF. Here is an example how to print few drawing per page. We start with importing and setting matpolotlib for PDF plotting. 1 2 3 4 import matplotlib matplotlib . use ( 'PDF' ) import matplotlib.pyplot as plt from matplotlib.backends.backend_pdf import PdfPages Next, we define subplot on a page organized by 3 rows of 2 figure each. Assuming that data is a list of tuples (title, values), we 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pdf = PdfPages ( 'pdffile.pdf' ) for i in range ( len ( data )): if i % 6 == 0 : # first in a page fig , axarr = plt . subplots ( 3 , 2 , figsize = ( 11 , 8.5 )) # data = [(title, x-values, y-values), ...] k = i % 6 ; fig = axarr [ int ( k / 2 ), k - int ( k / 2 ) * 2 ] title , x_values , y_values = data [ i ] fig . set_title ( title , fontsize = 11 ) fig . plot ( x_values , y_values , '+' ) if i % 6 == 5 : # last in a page fig . tight_layout ( pad = 0.4 , w_pad = 0.5 , h_pad = 1.0 ) pdf . savefig () plt . close () pdf . close () Moment moment is an IDL function that provides in a single call 4 statistical calculations. moment returns a four-element vector containing the mean, variance, skewness, and kurtosis of the input vector. Similar functionality can be accomplish using numpy and scipy. 1 2 3 4 5 6 7 8 9 from scipy import stats def moment ( value_vec ) mean = np . mean ( value_vec ) sdev = np . sqrt ( np . var ( value_vec )) skew = stats . skew ( value_vec ) vkurt = stats . kurtosis ( value_vec ) return mean , sdev , skew , vkurt scipy's stats_ has many other shortcut computations that are worth a while to look at. In reality, it is rare that one needs all four computational elements. Therefore, it is better to engage with the specific functions as needed instead of using moment(). Store and Recover It can be quite annoying to debug the last step in a multi-steps analytic were each step is a computation taking a long time. Well, once you pass the first step, debugging each consequent step can be annoying. What you want to do is to keep state of the multi-step process such that data would be stored after each completed step. That will allow you to jump into the step being debugged immediately. A simple mechanism to do that is to store the dataset at the end of each step. Before the step starts, the program can check if a result is stored for that step. If so, it recovers and skips the step. There needs to be a few flags for a program that does that. One --recoverable to enable the mechanism to store datasets. --recover to enable loading previously stored recovery datasets. Also, --recdir to set location for recovery datasets (obviously naming is merely a suggestion). 1 2 3 4 5 step_1_var_rec = None ; step_var_1 = None if recoverable : step_1_var_rec = Recovery ( 'step_var_1' , match_file ) if recover : step_var_1 = step_1_var_rec . load () 1 2 if step_1_var_rec and step_var_1 is not None : step_1_var_rec . store ( step_var_1 ) The code for Recovery shown here is a bit elaborated. Its sophistication arises from that it automates recovery based on time comparison of a source file ( assoc_path ) and the recoverable storage. This is done in the load function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Recovery ( object ): def __init__ ( self , name , assoc_path , location = None ): self . assoc_path = assoc_path self . name = name self . obj_file = self . get_obj_file ( location ) def get_obj_file ( self , location = None ): result = self . assoc_path + '. %s ' % self . name if location : name = os . path . basename ( result ) result = os . path . join ( location , name ) return result def load ( self ,): obj = None if self . obj_file : if os . path . isfile ( self . obj_file ): obj_file_m_time = os . path . getmtime ( self . obj_file ) assoc_path_m_time = 0 if os . path . isfile ( self . assoc_path ) or os . path . isdir ( self . assoc_path ): assoc_path_m_time = os . path . getmtime ( self . assoc_path ) if assoc_path_m_time > 0 and obj_file_m_time >= assoc_path_m_time : # not a new file, read goodobj from file print ( \"Recovering %s from %s \" % ( self . name , self . obj_file )) with open ( self . obj_file , 'rb' ) as f : obj = pickle . load ( f ) return obj def store ( self , obj ): print ( \"Storing %s into %s \" % ( self . name , self . obj_file )) with open ( self . obj_file , 'wb' ) as f : pickle . dump ( obj , f ) Interesting astrophysics packages Through the work, we ran into two Python packages for astronomy. Astropy : \"A Community Python Library for Astronomy.\" PyAstronomy : \"A collection of astronomy related packages.\" These packages contain plenty of easy to use functionality. However, be cautious when using in high volume processing. Not all computations may meet your need for performance. For example, we had to rewrite IDL's SIXTY_ instead of using an already made solution in these libraries. Only a scaled down version meet our performance needs. References IDL to Numeric/numarray Mapping NumPy for IDL users Ten Little IDL programs in Python The IDL Astronomy User's Library HARRIS's IDL Give us your feedback: support@acrisel.com"},{"url":"posts/2017/08/python-ordered-class/","title":"Python Ordered Class","tags":"08","text":"Maintain Ordered of Fields in Class Introduction We saw many questions on the WEB regarding having Python class maintain order of its fields. Reason being that __dict__ is dict, therefore, class doesn't register the order of the fields presented to it. We decided to share our version of such a class. We use OrderedClass were we need to maintain record like structure. Such that when we package the record, it always packages the fields in the same order. It is also useful when comparing fields between objects. Having fields being printed out in the same order is very useful. Keep in mind that without using OrderedDict, object __dict__ will print in order. However, Python does not guarantee that order. When using OrderedDict instead, order is guaranteed. OrderedClass Using metaclass features of Python we can simply override __prepare__ returning OrderedDict object. 1 2 3 4 5 6 7 8 9 from collections import OrderedDict class OrderedClassMeta ( type ): @classmethod def __prepare__ ( cls , name , bases , ** kwds ): return OrderedDict () class OrderedClass ( metaclass = OrderedClassMeta ): pass Example use To use, we just inherent from OrderedClass instead of object. 1 2 3 4 5 6 7 8 9 class A ( OrderedClass ): def __init__ ( self ): self . b = 1 self . a = 2 class B ( OrderedClass ): def __init__ ( self ): self . a = 1 self . b = 2 Examine Output Printing the dictionaries of the above examples. 1 2 3 4 a = A () print ( a . __dict__ ) b = B () print ( b . __dict__ ) 1 2 { 'b' : 1 , 'a' : 2 } { 'a' : 1 , 'b' : 2 } References Python PEP 520 Give us your feedback: support@acrisel.com Visit us at our home"},{"url":"posts/2017/08/python-print-directory-tree/","title":"Python Print Directory Tree","tags":"08","text":"Mimicking Linux Tree Utility Introduction Many blogs are showing how to print directory tree using Python. Drawing from those examples, we built our version. The primary drivers were: Compatibility with Python3 Print symbolic links Limit depth of tree Example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 $ ptree . py - l 2 / var var -> private / var / |-- venv -> / usr / local / share / virtualenvs |-- accord / | |-- . DS_Store | |-- data / | | |-- . DS_Store |-- acrisel / | |-- . DS_Store | |-- . gitignore | |-- accord / | | |-- . DS_Store Function Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import os def realname ( path , root = None ): if root is not None : path = os . path . join ( root , path ) result = os . path . basename ( path ) if os . path . islink ( path ): realpath = os . readlink ( path ) result = ' %s -> %s ' % ( os . path . basename ( path ), realpath ) return result def ptree ( startpath , depth =- 1 ): prefix = 0 if startpath != '/' : if startpath . endswith ( '/' ): startpath = startpath [: - 1 ] prefix = len ( startpath ) for root , dirs , files in os . walk ( startpath ): level = root [ prefix :] . count ( os . sep ) if depth >- 1 and level > depth : continue indent = subindent = '' if level > 0 : indent = '| ' * ( level - 1 ) + '|-- ' subindent = '| ' * ( level ) + '|-- ' print ( '{}{}/' . format ( indent , realname ( root ))) # print dir only if symbolic link; otherwise, will be printed as root for d in dirs : if os . path . islink ( os . path . join ( root , d )): print ( '{}{}' . format ( subindent , realname ( d , root = root ))) for f in files : print ( '{}{}' . format ( subindent , realname ( f , root = root ))) I will refrain from going over the code, otherwise self-explanatory, except mentioning the following: os.walk treats symbolic links per their target. Therefore, a symbolic link may appear in dirs and files. only two main features of tree are replicated: accepting both a root path and depth to explore. Command line Arguments Command line arguments is simple and self explanatory ... 1 2 3 4 5 6 7 8 9 10 11 if __name__ == '__main__' : import argparse parser = argparse . ArgumentParser ( description = 'prints directory tree.' ) parser . add_argument ( '--level' , '-l' , type = int , dest = 'depth' , default =- 1 , help = 'depth of tree to print' ) parser . add_argument ( 'startpath' , type = str , help = 'path to stating directory' ) args = parser . parse_args () argsd = vars ( args ) ptree ( ** argsd ) References ptree.py can be download from github Give us your feedback: support@acrisel.com Visit us at our home"},{"url":"posts/2017/07/cobol-conversion-take-1/","title":"Cobol Conversion Take 1","tags":"07","text":"Acrisel team has vast experience in Cobol conversion. We are sharing this knowledge with our community. Introduction Cobol is a structural language with many years of maturity. It was built for business use, and it is used as that for a few decades now. So why to move away from it? Let's start with that Cobol is a great language with many features for data processing. It is used in many core business processes. So core and critical that companies afraid to touch it due to the risk of impairing their business. Cobol has a solid environment for development, test, and run since late 50's. So really, why to convert? There is plenty of information with conflicting advice to either stay with or move out of Cobol. Rather try to weigh in on such discussion, here are a few drivers that lead companies to consider moving on. In reality, stay or convert is a business investment with long term vision. As such, it needs to be incorporated into your company's long-term business goals and objectives. Continue reading : Thinking on Cobol Conversion . Give us your feedback: support@acrisel.com Visit us at our home"},{"url":"posts/2017/07/welcome-all/","title":"","tags":"about us","text":"Welcome Y'all Welcome to Acrisel community blog. The Acrisel Team will keep the blog updated with latest experience in different domains of software engineering. The Acrisel Team reserves the right to surprise us with posts from other areas of life that the team finds interesting. You can contact us ar support@acrisel.com Visti our website: http://www.acrisel.com Keep tuned."}]}